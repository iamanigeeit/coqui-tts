{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "from TTS.utils.audio import AudioProcessor\n",
    "from TTS.config import load_config\n",
    "from TTS.tts.models import setup_model as setup_tts_model\n",
    "import torch\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "\n",
    "BASE_DIR = '/home/perry/PycharmProjects/TTS/recipes/ljspeech/prune/tacotron2_nomask/'\n",
    "ZERO_DIR = BASE_DIR + 'baseline/'\n",
    "ZERO_FILE = ZERO_DIR + \"checkpoint_100000.pth.tar\"\n",
    "CONFIG_PATH = ZERO_DIR + \"config.json\"\n",
    "DATA_DIR = \"/home/perry/PycharmProjects/TTS/recipes/ljspeech/LJSpeech-1.1/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "outputs": [],
   "source": [
    "plt.style.use('dark_background')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > Setting up Audio Processor...\n",
      " | > sample_rate:22050\n",
      " | > resample:False\n",
      " | > num_mels:80\n",
      " | > log_func:np.log\n",
      " | > min_level_db:-100\n",
      " | > frame_shift_ms:None\n",
      " | > frame_length_ms:None\n",
      " | > ref_level_db:20.0\n",
      " | > fft_size:1024\n",
      " | > power:1.5\n",
      " | > preemphasis:0.0\n",
      " | > griffin_lim_iters:60\n",
      " | > signal_norm:False\n",
      " | > symmetric_norm:True\n",
      " | > mel_fmin:0\n",
      " | > mel_fmax:8000.0\n",
      " | > spec_gain:1.0\n",
      " | > stft_pad_mode:reflect\n",
      " | > max_norm:4.0\n",
      " | > clip_norm:True\n",
      " | > do_trim_silence:True\n",
      " | > trim_db:60.0\n",
      " | > do_sound_norm:False\n",
      " | > do_amp_to_db_linear:True\n",
      " | > do_amp_to_db_mel:True\n",
      " | > stats_path:None\n",
      " | > base:2.718281828459045\n",
      " | > hop_length:256\n",
      " | > win_length:1024\n",
      " > Using model: tacotron2\n",
      " > Model's reduction rate `r` is set to: 1\n"
     ]
    }
   ],
   "source": [
    "C = load_config(CONFIG_PATH)\n",
    "ap = AudioProcessor(**C.audio)\n",
    "model = setup_tts_model(C)\n",
    "model.load_checkpoint(C, ZERO_FILE, eval=True)\n",
    "model = model.cuda()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding.weight torch.Size([200, 512])\n",
      "encoder.convolutions.0.convolution1d.weight torch.Size([512, 512, 5])\n",
      "encoder.convolutions.0.convolution1d.bias torch.Size([512])\n",
      "encoder.convolutions.0.batch_normalization.weight torch.Size([512])\n",
      "encoder.convolutions.0.batch_normalization.bias torch.Size([512])\n",
      "encoder.convolutions.1.convolution1d.weight torch.Size([512, 512, 5])\n",
      "encoder.convolutions.1.convolution1d.bias torch.Size([512])\n",
      "encoder.convolutions.1.batch_normalization.weight torch.Size([512])\n",
      "encoder.convolutions.1.batch_normalization.bias torch.Size([512])\n",
      "encoder.convolutions.2.convolution1d.weight torch.Size([512, 512, 5])\n",
      "encoder.convolutions.2.convolution1d.bias torch.Size([512])\n",
      "encoder.convolutions.2.batch_normalization.weight torch.Size([512])\n",
      "encoder.convolutions.2.batch_normalization.bias torch.Size([512])\n",
      "encoder.lstm.weight_ih_l0 torch.Size([1024, 512])\n",
      "encoder.lstm.weight_hh_l0 torch.Size([1024, 256])\n",
      "encoder.lstm.bias_ih_l0 torch.Size([1024])\n",
      "encoder.lstm.bias_hh_l0 torch.Size([1024])\n",
      "encoder.lstm.weight_ih_l0_reverse torch.Size([1024, 512])\n",
      "encoder.lstm.weight_hh_l0_reverse torch.Size([1024, 256])\n",
      "encoder.lstm.bias_ih_l0_reverse torch.Size([1024])\n",
      "encoder.lstm.bias_hh_l0_reverse torch.Size([1024])\n",
      "decoder.prenet.linear_layers.0.linear_layer.weight torch.Size([256, 80])\n",
      "decoder.prenet.linear_layers.1.linear_layer.weight torch.Size([256, 256])\n",
      "decoder.attention_rnn.weight_ih torch.Size([4096, 768])\n",
      "decoder.attention_rnn.weight_hh torch.Size([4096, 1024])\n",
      "decoder.attention_rnn.bias_ih torch.Size([4096])\n",
      "decoder.attention_rnn.bias_hh torch.Size([4096])\n",
      "decoder.attention.query_layer.weight torch.Size([128, 1024])\n",
      "decoder.attention.query_layer.bias torch.Size([128])\n",
      "decoder.attention.key_layer.weight torch.Size([168, 128])\n",
      "decoder.attention.static_filter_conv.weight torch.Size([8, 1, 21])\n",
      "decoder.attention.static_filter_layer.weight torch.Size([128, 8])\n",
      "decoder.attention.dynamic_filter_layer.weight torch.Size([128, 8])\n",
      "decoder.attention.dynamic_filter_layer.bias torch.Size([128])\n",
      "decoder.attention.v.weight torch.Size([1, 128])\n",
      "decoder.decoder_rnn.weight_ih torch.Size([4096, 1536])\n",
      "decoder.decoder_rnn.weight_hh torch.Size([4096, 1024])\n",
      "decoder.decoder_rnn.bias_ih torch.Size([4096])\n",
      "decoder.decoder_rnn.bias_hh torch.Size([4096])\n",
      "decoder.linear_projection.linear_layer.weight torch.Size([80, 1536])\n",
      "decoder.linear_projection.linear_layer.bias torch.Size([80])\n",
      "decoder.stopnet.1.linear_layer.weight torch.Size([1, 1104])\n",
      "decoder.stopnet.1.linear_layer.bias torch.Size([1])\n",
      "postnet.convolutions.0.convolution1d.weight torch.Size([512, 80, 5])\n",
      "postnet.convolutions.0.convolution1d.bias torch.Size([512])\n",
      "postnet.convolutions.0.batch_normalization.weight torch.Size([512])\n",
      "postnet.convolutions.0.batch_normalization.bias torch.Size([512])\n",
      "postnet.convolutions.1.convolution1d.weight torch.Size([512, 512, 5])\n",
      "postnet.convolutions.1.convolution1d.bias torch.Size([512])\n",
      "postnet.convolutions.1.batch_normalization.weight torch.Size([512])\n",
      "postnet.convolutions.1.batch_normalization.bias torch.Size([512])\n",
      "postnet.convolutions.2.convolution1d.weight torch.Size([512, 512, 5])\n",
      "postnet.convolutions.2.convolution1d.bias torch.Size([512])\n",
      "postnet.convolutions.2.batch_normalization.weight torch.Size([512])\n",
      "postnet.convolutions.2.batch_normalization.bias torch.Size([512])\n",
      "postnet.convolutions.3.convolution1d.weight torch.Size([512, 512, 5])\n",
      "postnet.convolutions.3.convolution1d.bias torch.Size([512])\n",
      "postnet.convolutions.3.batch_normalization.weight torch.Size([512])\n",
      "postnet.convolutions.3.batch_normalization.bias torch.Size([512])\n",
      "postnet.convolutions.4.convolution1d.weight torch.Size([80, 512, 5])\n",
      "postnet.convolutions.4.convolution1d.bias torch.Size([80])\n",
      "postnet.convolutions.4.batch_normalization.weight torch.Size([80])\n",
      "postnet.convolutions.4.batch_normalization.bias torch.Size([80])\n",
      "coarse_decoder.prenet.linear_layers.0.linear_layer.weight torch.Size([256, 80])\n",
      "coarse_decoder.prenet.linear_layers.1.linear_layer.weight torch.Size([256, 256])\n",
      "coarse_decoder.attention_rnn.weight_ih torch.Size([4096, 768])\n",
      "coarse_decoder.attention_rnn.weight_hh torch.Size([4096, 1024])\n",
      "coarse_decoder.attention_rnn.bias_ih torch.Size([4096])\n",
      "coarse_decoder.attention_rnn.bias_hh torch.Size([4096])\n",
      "coarse_decoder.attention.query_layer.weight torch.Size([128, 1024])\n",
      "coarse_decoder.attention.query_layer.bias torch.Size([128])\n",
      "coarse_decoder.attention.key_layer.weight torch.Size([168, 128])\n",
      "coarse_decoder.attention.static_filter_conv.weight torch.Size([8, 1, 21])\n",
      "coarse_decoder.attention.static_filter_layer.weight torch.Size([128, 8])\n",
      "coarse_decoder.attention.dynamic_filter_layer.weight torch.Size([128, 8])\n",
      "coarse_decoder.attention.dynamic_filter_layer.bias torch.Size([128])\n",
      "coarse_decoder.attention.v.weight torch.Size([1, 128])\n",
      "coarse_decoder.decoder_rnn.weight_ih torch.Size([4096, 1536])\n",
      "coarse_decoder.decoder_rnn.weight_hh torch.Size([4096, 1024])\n",
      "coarse_decoder.decoder_rnn.bias_ih torch.Size([4096])\n",
      "coarse_decoder.decoder_rnn.bias_hh torch.Size([4096])\n",
      "coarse_decoder.linear_projection.linear_layer.weight torch.Size([480, 1536])\n",
      "coarse_decoder.linear_projection.linear_layer.bias torch.Size([480])\n",
      "coarse_decoder.stopnet.1.linear_layer.weight torch.Size([1, 1504])\n",
      "coarse_decoder.stopnet.1.linear_layer.bias torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(name, param.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding.weight 102400 101888\n",
      "encoder.convolutions.0.convolution1d.weight 1310720 1310720\n",
      "encoder.convolutions.0.convolution1d.bias 512 512\n",
      "encoder.convolutions.0.batch_normalization.weight 512 512\n",
      "encoder.convolutions.0.batch_normalization.bias 512 512\n",
      "encoder.convolutions.1.convolution1d.weight 1310720 1310720\n",
      "encoder.convolutions.1.convolution1d.bias 512 512\n",
      "encoder.convolutions.1.batch_normalization.weight 512 512\n",
      "encoder.convolutions.1.batch_normalization.bias 512 512\n",
      "encoder.convolutions.2.convolution1d.weight 1310720 1310720\n",
      "encoder.convolutions.2.convolution1d.bias 512 512\n",
      "encoder.convolutions.2.batch_normalization.weight 512 512\n",
      "encoder.convolutions.2.batch_normalization.bias 512 512\n",
      "encoder.lstm.weight_ih_l0 524288 524288\n",
      "encoder.lstm.weight_hh_l0 262144 262144\n",
      "encoder.lstm.bias_ih_l0 1024 1024\n",
      "encoder.lstm.bias_hh_l0 1024 1024\n",
      "encoder.lstm.weight_ih_l0_reverse 524288 524288\n",
      "encoder.lstm.weight_hh_l0_reverse 262144 262144\n",
      "encoder.lstm.bias_ih_l0_reverse 1024 1024\n",
      "encoder.lstm.bias_hh_l0_reverse 1024 1024\n",
      "decoder.prenet.linear_layers.0.linear_layer.weight 20480 20480\n",
      "decoder.prenet.linear_layers.1.linear_layer.weight 65536 65536\n",
      "decoder.attention_rnn.weight_ih 3145728 3145728\n",
      "decoder.attention_rnn.weight_hh 4194304 4194304\n",
      "decoder.attention_rnn.bias_ih 4096 4096\n",
      "decoder.attention_rnn.bias_hh 4096 4096\n",
      "decoder.attention.query_layer.weight 131072 131072\n",
      "decoder.attention.query_layer.bias 128 128\n",
      "decoder.attention.key_layer.weight 21504 21504\n",
      "decoder.attention.static_filter_conv.weight 168 168\n",
      "decoder.attention.static_filter_layer.weight 1024 1024\n",
      "decoder.attention.dynamic_filter_layer.weight 1024 1024\n",
      "decoder.attention.dynamic_filter_layer.bias 128 128\n",
      "decoder.attention.v.weight 128 128\n",
      "decoder.decoder_rnn.weight_ih 6291456 6291456\n",
      "decoder.decoder_rnn.weight_hh 4194304 4194304\n",
      "decoder.decoder_rnn.bias_ih 4096 4096\n",
      "decoder.decoder_rnn.bias_hh 4096 4096\n",
      "decoder.linear_projection.linear_layer.weight 122880 122880\n",
      "decoder.linear_projection.linear_layer.bias 80 80\n",
      "decoder.stopnet.1.linear_layer.weight 1104 1104\n",
      "decoder.stopnet.1.linear_layer.bias 1 1\n",
      "postnet.convolutions.0.convolution1d.weight 204800 204800\n",
      "postnet.convolutions.0.convolution1d.bias 512 512\n",
      "postnet.convolutions.0.batch_normalization.weight 512 512\n",
      "postnet.convolutions.0.batch_normalization.bias 512 512\n",
      "postnet.convolutions.1.convolution1d.weight 1310720 1310720\n",
      "postnet.convolutions.1.convolution1d.bias 512 512\n",
      "postnet.convolutions.1.batch_normalization.weight 512 512\n",
      "postnet.convolutions.1.batch_normalization.bias 512 512\n",
      "postnet.convolutions.2.convolution1d.weight 1310720 1310720\n",
      "postnet.convolutions.2.convolution1d.bias 512 512\n",
      "postnet.convolutions.2.batch_normalization.weight 512 512\n",
      "postnet.convolutions.2.batch_normalization.bias 512 512\n",
      "postnet.convolutions.3.convolution1d.weight 1310720 1310720\n",
      "postnet.convolutions.3.convolution1d.bias 512 512\n",
      "postnet.convolutions.3.batch_normalization.weight 512 512\n",
      "postnet.convolutions.3.batch_normalization.bias 512 512\n",
      "postnet.convolutions.4.convolution1d.weight 204800 204800\n",
      "postnet.convolutions.4.convolution1d.bias 80 80\n",
      "postnet.convolutions.4.batch_normalization.weight 80 80\n",
      "postnet.convolutions.4.batch_normalization.bias 80 80\n",
      "coarse_decoder.prenet.linear_layers.0.linear_layer.weight 20480 20480\n",
      "coarse_decoder.prenet.linear_layers.1.linear_layer.weight 65536 65536\n",
      "coarse_decoder.attention_rnn.weight_ih 3145728 3145728\n",
      "coarse_decoder.attention_rnn.weight_hh 4194304 4194304\n",
      "coarse_decoder.attention_rnn.bias_ih 4096 4096\n",
      "coarse_decoder.attention_rnn.bias_hh 4096 4096\n",
      "coarse_decoder.attention.query_layer.weight 131072 131072\n",
      "coarse_decoder.attention.query_layer.bias 128 128\n",
      "coarse_decoder.attention.key_layer.weight 21504 21504\n",
      "coarse_decoder.attention.static_filter_conv.weight 168 168\n",
      "coarse_decoder.attention.static_filter_layer.weight 1024 1024\n",
      "coarse_decoder.attention.dynamic_filter_layer.weight 1024 1024\n",
      "coarse_decoder.attention.dynamic_filter_layer.bias 128 128\n",
      "coarse_decoder.attention.v.weight 128 128\n",
      "coarse_decoder.decoder_rnn.weight_ih 6291456 6291456\n",
      "coarse_decoder.decoder_rnn.weight_hh 4194304 4194304\n",
      "coarse_decoder.decoder_rnn.bias_ih 4096 4096\n",
      "coarse_decoder.decoder_rnn.bias_hh 4096 4096\n",
      "coarse_decoder.linear_projection.linear_layer.weight 737280 737280\n",
      "coarse_decoder.linear_projection.linear_layer.bias 480 480\n",
      "coarse_decoder.stopnet.1.linear_layer.weight 1504 1504\n",
      "coarse_decoder.stopnet.1.linear_layer.bias 1 1\n"
     ]
    }
   ],
   "source": [
    "for n, p in model.named_parameters():\n",
    "    print(n, p.numel(), p.count_nonzero().item())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "outputs": [],
   "source": [
    "from TTS.utils.generic_utils import get_module_by_name\n",
    "param_prune_dim = {'embedding.weight': 1,\n",
    "                   'encoder.convolutions.0.convolution1d.weight': 0,\n",
    "                   'encoder.convolutions.1.convolution1d.weight': 0,\n",
    "                   'encoder.convolutions.2.convolution1d.weight': 0,\n",
    "                   'encoder.lstm.weight_ih_l0': 0,\n",
    "                   'encoder.lstm.weight_hh_l0': 0,\n",
    "                   'encoder.lstm.weight_ih_l0_reverse': 0,\n",
    "                   'encoder.lstm.weight_hh_l0_reverse': 0,\n",
    "                   'decoder.prenet.linear_layers.0.linear_layer.weight': 0,\n",
    "                   'decoder.attention_rnn.weight_ih': 0,\n",
    "                   'decoder.attention_rnn.weight_hh': 0,\n",
    "                   'decoder.attention.query_layer.weight': 0,\n",
    "                   'decoder.attention.key_layer.weight': 0,\n",
    "                   'decoder.decoder_rnn.weight_ih': 0,\n",
    "                   'decoder.decoder_rnn.weight_hh': 0,\n",
    "                   'postnet.convolutions.0.convolution1d.weight': 0,\n",
    "                   'postnet.convolutions.1.convolution1d.weight': 0,\n",
    "                   'postnet.convolutions.2.convolution1d.weight': 0,\n",
    "                   'postnet.convolutions.3.convolution1d.weight': 0,\n",
    "                   'coarse_decoder.prenet.linear_layers.0.linear_layer.weight': 0,\n",
    "                   'coarse_decoder.prenet.linear_layers.1.linear_layer.weight': 0,\n",
    "                   'coarse_decoder.attention_rnn.weight_ih': 0,\n",
    "                   'coarse_decoder.attention_rnn.weight_hh': 0,\n",
    "                   'coarse_decoder.attention.query_layer.weight': 0,\n",
    "                   'coarse_decoder.attention.key_layer.weight': 0,\n",
    "                   'coarse_decoder.decoder_rnn.weight_ih': 0,\n",
    "                   'coarse_decoder.decoder_rnn.weight_hh': 0,\n",
    "                    }"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > Using model: tacotron2\n",
      " > Model's reduction rate `r` is set to: 1\n"
     ]
    }
   ],
   "source": [
    "UMP_DIR = BASE_DIR + 'ump/'\n",
    "UMP_10_FILE = UMP_DIR + 'sparsity_10_stop/checkpoint_100000.pth.tar'\n",
    "config = load_config(CONFIG_PATH)\n",
    "ump_model = setup_tts_model(config)\n",
    "ump_model.load_checkpoint(config, UMP_10_FILE, eval=True)\n",
    "ump_model = ump_model.cuda()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding.weight torch.Size([200, 512])\n",
      "encoder.convolutions.0.convolution1d.weight torch.Size([512, 512, 5])\n",
      "encoder.convolutions.0.convolution1d.bias torch.Size([512])\n",
      "encoder.convolutions.0.batch_normalization.weight torch.Size([512])\n",
      "encoder.convolutions.0.batch_normalization.bias torch.Size([512])\n",
      "encoder.convolutions.1.convolution1d.weight torch.Size([512, 512, 5])\n",
      "encoder.convolutions.1.convolution1d.bias torch.Size([512])\n",
      "encoder.convolutions.1.batch_normalization.weight torch.Size([512])\n",
      "encoder.convolutions.1.batch_normalization.bias torch.Size([512])\n",
      "encoder.convolutions.2.convolution1d.weight torch.Size([512, 512, 5])\n",
      "encoder.convolutions.2.convolution1d.bias torch.Size([512])\n",
      "encoder.convolutions.2.batch_normalization.weight torch.Size([512])\n",
      "encoder.convolutions.2.batch_normalization.bias torch.Size([512])\n",
      "encoder.lstm.weight_ih_l0 torch.Size([1024, 512])\n",
      "encoder.lstm.weight_hh_l0 torch.Size([1024, 256])\n",
      "encoder.lstm.bias_ih_l0 torch.Size([1024])\n",
      "encoder.lstm.bias_hh_l0 torch.Size([1024])\n",
      "encoder.lstm.weight_ih_l0_reverse torch.Size([1024, 512])\n",
      "encoder.lstm.weight_hh_l0_reverse torch.Size([1024, 256])\n",
      "encoder.lstm.bias_ih_l0_reverse torch.Size([1024])\n",
      "encoder.lstm.bias_hh_l0_reverse torch.Size([1024])\n",
      "decoder.prenet.linear_layers.0.linear_layer.weight torch.Size([256, 80])\n",
      "decoder.prenet.linear_layers.1.linear_layer.weight torch.Size([256, 256])\n",
      "decoder.attention_rnn.weight_ih torch.Size([4096, 768])\n",
      "decoder.attention_rnn.weight_hh torch.Size([4096, 1024])\n",
      "decoder.attention_rnn.bias_ih torch.Size([4096])\n",
      "decoder.attention_rnn.bias_hh torch.Size([4096])\n",
      "decoder.attention.query_layer.weight torch.Size([128, 1024])\n",
      "decoder.attention.query_layer.bias torch.Size([128])\n",
      "decoder.attention.key_layer.weight torch.Size([168, 128])\n",
      "decoder.attention.static_filter_conv.weight torch.Size([8, 1, 21])\n",
      "decoder.attention.static_filter_layer.weight torch.Size([128, 8])\n",
      "decoder.attention.dynamic_filter_layer.weight torch.Size([128, 8])\n",
      "decoder.attention.dynamic_filter_layer.bias torch.Size([128])\n",
      "decoder.attention.v.weight torch.Size([1, 128])\n",
      "decoder.decoder_rnn.weight_ih torch.Size([4096, 1536])\n",
      "decoder.decoder_rnn.weight_hh torch.Size([4096, 1024])\n",
      "decoder.decoder_rnn.bias_ih torch.Size([4096])\n",
      "decoder.decoder_rnn.bias_hh torch.Size([4096])\n",
      "decoder.linear_projection.linear_layer.weight torch.Size([80, 1536])\n",
      "decoder.linear_projection.linear_layer.bias torch.Size([80])\n",
      "decoder.stopnet.1.linear_layer.weight torch.Size([1, 1104])\n",
      "decoder.stopnet.1.linear_layer.bias torch.Size([1])\n",
      "postnet.convolutions.0.convolution1d.weight torch.Size([512, 80, 5])\n",
      "postnet.convolutions.0.convolution1d.bias torch.Size([512])\n",
      "postnet.convolutions.0.batch_normalization.weight torch.Size([512])\n",
      "postnet.convolutions.0.batch_normalization.bias torch.Size([512])\n",
      "postnet.convolutions.1.convolution1d.weight torch.Size([512, 512, 5])\n",
      "postnet.convolutions.1.convolution1d.bias torch.Size([512])\n",
      "postnet.convolutions.1.batch_normalization.weight torch.Size([512])\n",
      "postnet.convolutions.1.batch_normalization.bias torch.Size([512])\n",
      "postnet.convolutions.2.convolution1d.weight torch.Size([512, 512, 5])\n",
      "postnet.convolutions.2.convolution1d.bias torch.Size([512])\n",
      "postnet.convolutions.2.batch_normalization.weight torch.Size([512])\n",
      "postnet.convolutions.2.batch_normalization.bias torch.Size([512])\n",
      "postnet.convolutions.3.convolution1d.weight torch.Size([512, 512, 5])\n",
      "postnet.convolutions.3.convolution1d.bias torch.Size([512])\n",
      "postnet.convolutions.3.batch_normalization.weight torch.Size([512])\n",
      "postnet.convolutions.3.batch_normalization.bias torch.Size([512])\n",
      "postnet.convolutions.4.convolution1d.weight torch.Size([80, 512, 5])\n",
      "postnet.convolutions.4.convolution1d.bias torch.Size([80])\n",
      "postnet.convolutions.4.batch_normalization.weight torch.Size([80])\n",
      "postnet.convolutions.4.batch_normalization.bias torch.Size([80])\n",
      "coarse_decoder.prenet.linear_layers.0.linear_layer.weight torch.Size([256, 80])\n",
      "coarse_decoder.prenet.linear_layers.1.linear_layer.weight torch.Size([256, 256])\n",
      "coarse_decoder.attention_rnn.weight_ih torch.Size([4096, 768])\n",
      "coarse_decoder.attention_rnn.weight_hh torch.Size([4096, 1024])\n",
      "coarse_decoder.attention_rnn.bias_ih torch.Size([4096])\n",
      "coarse_decoder.attention_rnn.bias_hh torch.Size([4096])\n",
      "coarse_decoder.attention.query_layer.weight torch.Size([128, 1024])\n",
      "coarse_decoder.attention.query_layer.bias torch.Size([128])\n",
      "coarse_decoder.attention.key_layer.weight torch.Size([168, 128])\n",
      "coarse_decoder.attention.static_filter_conv.weight torch.Size([8, 1, 21])\n",
      "coarse_decoder.attention.static_filter_layer.weight torch.Size([128, 8])\n",
      "coarse_decoder.attention.dynamic_filter_layer.weight torch.Size([128, 8])\n",
      "coarse_decoder.attention.dynamic_filter_layer.bias torch.Size([128])\n",
      "coarse_decoder.attention.v.weight torch.Size([1, 128])\n",
      "coarse_decoder.decoder_rnn.weight_ih torch.Size([4096, 1536])\n",
      "coarse_decoder.decoder_rnn.weight_hh torch.Size([4096, 1024])\n",
      "coarse_decoder.decoder_rnn.bias_ih torch.Size([4096])\n",
      "coarse_decoder.decoder_rnn.bias_hh torch.Size([4096])\n",
      "coarse_decoder.linear_projection.linear_layer.weight torch.Size([480, 1536])\n",
      "coarse_decoder.linear_projection.linear_layer.bias torch.Size([480])\n",
      "coarse_decoder.stopnet.1.linear_layer.weight torch.Size([1, 1504])\n",
      "coarse_decoder.stopnet.1.linear_layer.bias torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "for n, p in ump_model.named_parameters():\n",
    "    print(n, p.size())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "outputs": [],
   "source": [
    "ump_params = {n: (p.numel(), p.count_nonzero().item()) for n, p in ump_model.named_parameters()}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "outputs": [
    {
     "data": {
      "text/plain": "{'embedding.weight': (102400, 101616),\n 'encoder.convolutions.0.convolution1d.weight': (1310720, 1110363),\n 'encoder.convolutions.0.convolution1d.bias': (512, 417),\n 'encoder.convolutions.0.batch_normalization.weight': (512, 512),\n 'encoder.convolutions.0.batch_normalization.bias': (512, 512),\n 'encoder.convolutions.1.convolution1d.weight': (1310720, 1098397),\n 'encoder.convolutions.1.convolution1d.bias': (512, 437),\n 'encoder.convolutions.1.batch_normalization.weight': (512, 512),\n 'encoder.convolutions.1.batch_normalization.bias': (512, 512),\n 'encoder.convolutions.2.convolution1d.weight': (1310720, 1091901),\n 'encoder.convolutions.2.convolution1d.bias': (512, 424),\n 'encoder.convolutions.2.batch_normalization.weight': (512, 512),\n 'encoder.convolutions.2.batch_normalization.bias': (512, 512),\n 'encoder.lstm.weight_ih_l0': (524288, 492748),\n 'encoder.lstm.weight_hh_l0': (262144, 248566),\n 'encoder.lstm.bias_ih_l0': (1024, 966),\n 'encoder.lstm.bias_hh_l0': (1024, 987),\n 'encoder.lstm.weight_ih_l0_reverse': (524288, 493396),\n 'encoder.lstm.weight_hh_l0_reverse': (262144, 248346),\n 'encoder.lstm.bias_ih_l0_reverse': (1024, 976),\n 'encoder.lstm.bias_hh_l0_reverse': (1024, 976),\n 'decoder.prenet.linear_layers.0.linear_layer.weight': (20480, 19952),\n 'decoder.prenet.linear_layers.1.linear_layer.weight': (65536, 62295),\n 'decoder.attention_rnn.weight_ih': (3145728, 2818330),\n 'decoder.attention_rnn.weight_hh': (4194304, 3825083),\n 'decoder.attention_rnn.bias_ih': (4096, 3712),\n 'decoder.attention_rnn.bias_hh': (4096, 3706),\n 'decoder.attention.query_layer.weight': (131072, 117939),\n 'decoder.attention.query_layer.bias': (128, 112),\n 'decoder.attention.key_layer.weight': (21504, 20732),\n 'decoder.attention.static_filter_conv.weight': (168, 167),\n 'decoder.attention.static_filter_layer.weight': (1024, 1017),\n 'decoder.attention.dynamic_filter_layer.weight': (1024, 1014),\n 'decoder.attention.dynamic_filter_layer.bias': (128, 128),\n 'decoder.attention.v.weight': (128, 126),\n 'decoder.decoder_rnn.weight_ih': (6291456, 5689430),\n 'decoder.decoder_rnn.weight_hh': (4194304, 3880764),\n 'decoder.decoder_rnn.bias_ih': (4096, 3641),\n 'decoder.decoder_rnn.bias_hh': (4096, 3632),\n 'decoder.linear_projection.linear_layer.weight': (122880, 109817),\n 'decoder.linear_projection.linear_layer.bias': (80, 65),\n 'decoder.stopnet.1.linear_layer.weight': (1104, 1104),\n 'decoder.stopnet.1.linear_layer.bias': (1, 1),\n 'postnet.convolutions.0.convolution1d.weight': (204800, 190869),\n 'postnet.convolutions.0.convolution1d.bias': (512, 484),\n 'postnet.convolutions.0.batch_normalization.weight': (512, 512),\n 'postnet.convolutions.0.batch_normalization.bias': (512, 493),\n 'postnet.convolutions.1.convolution1d.weight': (1310720, 1078242),\n 'postnet.convolutions.1.convolution1d.bias': (512, 426),\n 'postnet.convolutions.1.batch_normalization.weight': (512, 512),\n 'postnet.convolutions.1.batch_normalization.bias': (512, 503),\n 'postnet.convolutions.2.convolution1d.weight': (1310720, 1081297),\n 'postnet.convolutions.2.convolution1d.bias': (512, 432),\n 'postnet.convolutions.2.batch_normalization.weight': (512, 512),\n 'postnet.convolutions.2.batch_normalization.bias': (512, 497),\n 'postnet.convolutions.3.convolution1d.weight': (1310720, 1079702),\n 'postnet.convolutions.3.convolution1d.bias': (512, 431),\n 'postnet.convolutions.3.batch_normalization.weight': (512, 512),\n 'postnet.convolutions.3.batch_normalization.bias': (512, 500),\n 'postnet.convolutions.4.convolution1d.weight': (204800, 170592),\n 'postnet.convolutions.4.convolution1d.bias': (80, 69),\n 'postnet.convolutions.4.batch_normalization.weight': (80, 80),\n 'postnet.convolutions.4.batch_normalization.bias': (80, 80),\n 'coarse_decoder.prenet.linear_layers.0.linear_layer.weight': (20480, 19926),\n 'coarse_decoder.prenet.linear_layers.1.linear_layer.weight': (65536, 62918),\n 'coarse_decoder.attention_rnn.weight_ih': (3145728, 2835437),\n 'coarse_decoder.attention_rnn.weight_hh': (4194304, 3792005),\n 'coarse_decoder.attention_rnn.bias_ih': (4096, 3739),\n 'coarse_decoder.attention_rnn.bias_hh': (4096, 3730),\n 'coarse_decoder.attention.query_layer.weight': (131072, 118621),\n 'coarse_decoder.attention.query_layer.bias': (128, 112),\n 'coarse_decoder.attention.key_layer.weight': (21504, 20703),\n 'coarse_decoder.attention.static_filter_conv.weight': (168, 168),\n 'coarse_decoder.attention.static_filter_layer.weight': (1024, 1008),\n 'coarse_decoder.attention.dynamic_filter_layer.weight': (1024, 1017),\n 'coarse_decoder.attention.dynamic_filter_layer.bias': (128, 126),\n 'coarse_decoder.attention.v.weight': (128, 128),\n 'coarse_decoder.decoder_rnn.weight_ih': (6291456, 5762312),\n 'coarse_decoder.decoder_rnn.weight_hh': (4194304, 3928657),\n 'coarse_decoder.decoder_rnn.bias_ih': (4096, 3679),\n 'coarse_decoder.decoder_rnn.bias_hh': (4096, 3646),\n 'coarse_decoder.linear_projection.linear_layer.weight': (737280, 672204),\n 'coarse_decoder.linear_projection.linear_layer.bias': (480, 424),\n 'coarse_decoder.stopnet.1.linear_layer.weight': (1504, 1504),\n 'coarse_decoder.stopnet.1.linear_layer.bias': (1, 1)}"
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ump_params"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "outputs": [],
   "source": [
    "units_to_prune = {}\n",
    "for name in param_prune_dim:\n",
    "    numel, nonzero = ump_params[name]\n",
    "    units_to_prune[name] = (1 - nonzero / numel) / 10"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "outputs": [
    {
     "data": {
      "text/plain": "{'embedding.weight': 0.0007656250000000031,\n 'encoder.convolutions.0.convolution1d.weight': 0.01528602600097656,\n 'encoder.convolutions.1.convolution1d.weight': 0.01619895935058594,\n 'encoder.convolutions.2.convolution1d.weight': 0.016694564819335934,\n 'encoder.lstm.weight_ih_l0': 0.006015777587890625,\n 'encoder.lstm.weight_hh_l0': 0.005179595947265625,\n 'encoder.lstm.weight_ih_l0_reverse': 0.005892181396484375,\n 'encoder.lstm.weight_hh_l0_reverse': 0.005263519287109375,\n 'decoder.prenet.linear_layers.0.linear_layer.weight': 0.002578124999999998,\n 'decoder.attention_rnn.weight_ih': 0.010407702128092444,\n 'decoder.attention_rnn.weight_hh': 0.0088029146194458,\n 'decoder.attention.query_layer.weight': 0.010019683837890625,\n 'decoder.attention.key_layer.weight': 0.0035900297619047674,\n 'decoder.decoder_rnn.weight_ih': 0.009568945566813147,\n 'decoder.decoder_rnn.weight_hh': 0.007475376129150391,\n 'postnet.convolutions.0.convolution1d.weight': 0.006802246093749997,\n 'postnet.convolutions.1.convolution1d.weight': 0.017736663818359377,\n 'postnet.convolutions.2.convolution1d.weight': 0.01750358581542969,\n 'postnet.convolutions.3.convolution1d.weight': 0.017625274658203127,\n 'coarse_decoder.prenet.linear_layers.0.linear_layer.weight': 0.0027050781250000044,\n 'coarse_decoder.prenet.linear_layers.1.linear_layer.weight': 0.0039947509765625,\n 'coarse_decoder.attention_rnn.weight_ih': 0.009863885243733727,\n 'coarse_decoder.attention_rnn.weight_hh': 0.00959155559539795,\n 'coarse_decoder.attention.query_layer.weight': 0.009499359130859374,\n 'coarse_decoder.attention.key_layer.weight': 0.0037248883928571396,\n 'coarse_decoder.decoder_rnn.weight_ih': 0.008410517374674475,\n 'coarse_decoder.decoder_rnn.weight_hh': 0.0063335180282592775}"
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "units_to_prune"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruning embedding.weight: 512 channels at ratio 0.0007656250000000031)\n",
      "Pruning encoder.convolutions.0.convolution1d.weight: 512 channels at ratio 0.01528602600097656)\n",
      "Pruning encoder.convolutions.0.convolution1d.bias\n",
      "Pruning encoder.convolutions.1.convolution1d.weight: 512 channels at ratio 0.01619895935058594)\n",
      "Pruning encoder.convolutions.1.convolution1d.bias\n",
      "Pruning encoder.convolutions.2.convolution1d.weight: 512 channels at ratio 0.016694564819335934)\n",
      "Pruning encoder.convolutions.2.convolution1d.bias\n",
      "Pruning encoder.lstm.weight_ih_l0: 1024 channels at ratio 0.006015777587890625)\n",
      "Pruning encoder.lstm.bias_ih_l0\n",
      "Pruning encoder.lstm.weight_hh_l0: 1024 channels at ratio 0.005179595947265625)\n",
      "Pruning encoder.lstm.bias_hh_l0\n",
      "Pruning encoder.lstm.weight_ih_l0_reverse: 1024 channels at ratio 0.005892181396484375)\n",
      "Pruning encoder.lstm.bias_ih_l0_reverse\n",
      "Pruning encoder.lstm.weight_hh_l0_reverse: 1024 channels at ratio 0.005263519287109375)\n",
      "Pruning encoder.lstm.bias_hh_l0_reverse\n",
      "Pruning decoder.prenet.linear_layers.0.linear_layer.weight: 256 channels at ratio 0.002578124999999998)\n",
      "Pruning decoder.attention_rnn.weight_ih: 4096 channels at ratio 0.010407702128092444)\n",
      "Pruning decoder.attention_rnn.bias_ih\n",
      "Pruning decoder.attention_rnn.weight_hh: 4096 channels at ratio 0.0088029146194458)\n",
      "Pruning decoder.attention_rnn.bias_hh\n",
      "Pruning decoder.attention.query_layer.weight: 128 channels at ratio 0.010019683837890625)\n",
      "Pruning decoder.attention.query_layer.bias\n",
      "Pruning decoder.attention.key_layer.weight: 168 channels at ratio 0.0035900297619047674)\n",
      "Pruning decoder.decoder_rnn.weight_ih: 4096 channels at ratio 0.009568945566813147)\n",
      "Pruning decoder.decoder_rnn.bias_ih\n",
      "Pruning decoder.decoder_rnn.weight_hh: 4096 channels at ratio 0.007475376129150391)\n",
      "Pruning decoder.decoder_rnn.bias_hh\n",
      "Pruning postnet.convolutions.0.convolution1d.weight: 512 channels at ratio 0.006802246093749997)\n",
      "Pruning postnet.convolutions.0.convolution1d.bias\n",
      "Pruning postnet.convolutions.1.convolution1d.weight: 512 channels at ratio 0.017736663818359377)\n",
      "Pruning postnet.convolutions.1.convolution1d.bias\n",
      "Pruning postnet.convolutions.2.convolution1d.weight: 512 channels at ratio 0.01750358581542969)\n",
      "Pruning postnet.convolutions.2.convolution1d.bias\n",
      "Pruning postnet.convolutions.3.convolution1d.weight: 512 channels at ratio 0.017625274658203127)\n",
      "Pruning postnet.convolutions.3.convolution1d.bias\n",
      "Pruning coarse_decoder.prenet.linear_layers.0.linear_layer.weight: 256 channels at ratio 0.0027050781250000044)\n",
      "Pruning coarse_decoder.prenet.linear_layers.1.linear_layer.weight: 256 channels at ratio 0.0039947509765625)\n",
      "Pruning coarse_decoder.attention_rnn.weight_ih: 4096 channels at ratio 0.009863885243733727)\n",
      "Pruning coarse_decoder.attention_rnn.bias_ih\n",
      "Pruning coarse_decoder.attention_rnn.weight_hh: 4096 channels at ratio 0.00959155559539795)\n",
      "Pruning coarse_decoder.attention_rnn.bias_hh\n",
      "Pruning coarse_decoder.attention.query_layer.weight: 128 channels at ratio 0.009499359130859374)\n",
      "Pruning coarse_decoder.attention.query_layer.bias\n",
      "Pruning coarse_decoder.attention.key_layer.weight: 168 channels at ratio 0.0037248883928571396)\n",
      "Pruning coarse_decoder.decoder_rnn.weight_ih: 4096 channels at ratio 0.008410517374674475)\n",
      "Pruning coarse_decoder.decoder_rnn.bias_ih\n",
      "Pruning coarse_decoder.decoder_rnn.weight_hh: 4096 channels at ratio 0.0063335180282592775)\n",
      "Pruning coarse_decoder.decoder_rnn.bias_hh\n"
     ]
    }
   ],
   "source": [
    "from torch.nn.utils import prune\n",
    "for name, amount in units_to_prune.items():\n",
    "    m, n = name.rsplit('.', 1)\n",
    "    module = get_module_by_name(model, m)\n",
    "    dim = param_prune_dim[name]\n",
    "    param = getattr(module, n)\n",
    "    print(f'Pruning {name}: {param.shape[dim]} channels at ratio {amount}')\n",
    "    if param.shape[dim] * amount > 0.5:\n",
    "        prune.ln_structured(module, n, amount, 1, dim, importance_scores=None)\n",
    "        prune.remove(module, n)\n",
    "        bias_name = n.replace('weight', 'bias')\n",
    "        if hasattr(module, bias_name):\n",
    "            bias = getattr(module, bias_name)\n",
    "            if bias is not None:\n",
    "                print(f'Pruning {m}.{bias_name}')\n",
    "                add_axes = list(range(param.dim()))\n",
    "                add_axes.pop(dim)\n",
    "                param_sum = param.abs().sum(axis=add_axes)\n",
    "                bias = bias * (param_sum > 0.0)\n",
    "                bias = torch.nn.Parameter(bias)\n",
    "                setattr(module, bias_name, bias)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([512, 512, 5])"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from TTS.utils.generic_utils import get_param_by_name\n",
    "param = get_param_by_name(model, 'encoder.convolutions.0.convolution1d.weight')\n",
    "param.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "param_sum = param.abs().sum(axis=[1,2])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [],
   "source": [
    "bias = get_param_by_name(model, 'encoder.convolutions.0.convolution1d.bias')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [],
   "source": [
    "bias = bias * (param_sum > 0.0)\n",
    "bias = torch.nn.Parameter(bias)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [],
   "source": [
    "m, n = 'encoder.convolutions.0.convolution1d.bias'.rsplit('.', 1)\n",
    "module = get_module_by_name(model, m)\n",
    "setattr(module, n, bias)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [
    {
     "data": {
      "text/plain": "False"
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hasattr(module, 'birras')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "total_removed = 0\n",
    "for n, b in model.named_buffers():\n",
    "    if 'mask' in n:\n",
    "        removed = (b.numel() - b.count_nonzero().item())\n",
    "        total_removed += removed\n",
    "        print(n, removed)\n",
    "print(total_removed)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding.weight 102400 101888\n",
      "encoder.convolutions.0.convolution1d.bias 512 504\n",
      "encoder.convolutions.0.convolution1d.weight 1310720 1290240\n",
      "encoder.convolutions.0.batch_normalization.weight 512 512\n",
      "encoder.convolutions.0.batch_normalization.bias 512 512\n",
      "encoder.convolutions.1.convolution1d.bias 512 504\n",
      "encoder.convolutions.1.convolution1d.weight 1310720 1290240\n",
      "encoder.convolutions.1.batch_normalization.weight 512 512\n",
      "encoder.convolutions.1.batch_normalization.bias 512 512\n",
      "encoder.convolutions.2.convolution1d.bias 512 503\n",
      "encoder.convolutions.2.convolution1d.weight 1310720 1287680\n",
      "encoder.convolutions.2.batch_normalization.weight 512 512\n",
      "encoder.convolutions.2.batch_normalization.bias 512 512\n",
      "encoder.lstm.bias_ih_l0 1024 1018\n",
      "encoder.lstm.bias_hh_l0 1024 1019\n",
      "encoder.lstm.bias_ih_l0_reverse 1024 1018\n",
      "encoder.lstm.bias_hh_l0_reverse 1024 1019\n",
      "encoder.lstm.weight_ih_l0 524288 521216\n",
      "encoder.lstm.weight_hh_l0 262144 260864\n",
      "encoder.lstm.weight_ih_l0_reverse 524288 521216\n",
      "encoder.lstm.weight_hh_l0_reverse 262144 260864\n",
      "decoder.prenet.linear_layers.0.linear_layer.weight 20480 20400\n",
      "decoder.prenet.linear_layers.1.linear_layer.weight 65536 65536\n",
      "decoder.attention_rnn.bias_ih 4096 4053\n",
      "decoder.attention_rnn.bias_hh 4096 4060\n",
      "decoder.attention_rnn.weight_ih 3145728 3112704\n",
      "decoder.attention_rnn.weight_hh 4194304 4157440\n",
      "decoder.attention.query_layer.bias 128 127\n",
      "decoder.attention.query_layer.weight 131072 130048\n",
      "decoder.attention.key_layer.weight 21504 21376\n",
      "decoder.attention.static_filter_conv.weight 168 168\n",
      "decoder.attention.static_filter_layer.weight 1024 1024\n",
      "decoder.attention.dynamic_filter_layer.weight 1024 1024\n",
      "decoder.attention.dynamic_filter_layer.bias 128 128\n",
      "decoder.attention.v.weight 128 128\n",
      "decoder.decoder_rnn.bias_ih 4096 4057\n",
      "decoder.decoder_rnn.bias_hh 4096 4065\n",
      "decoder.decoder_rnn.weight_ih 6291456 6231552\n",
      "decoder.decoder_rnn.weight_hh 4194304 4162560\n",
      "decoder.linear_projection.linear_layer.weight 122880 122880\n",
      "decoder.linear_projection.linear_layer.bias 80 80\n",
      "decoder.stopnet.1.linear_layer.weight 1104 1104\n",
      "decoder.stopnet.1.linear_layer.bias 1 1\n",
      "postnet.convolutions.0.convolution1d.bias 512 509\n",
      "postnet.convolutions.0.convolution1d.weight 204800 203600\n",
      "postnet.convolutions.0.batch_normalization.weight 512 512\n",
      "postnet.convolutions.0.batch_normalization.bias 512 512\n",
      "postnet.convolutions.1.convolution1d.bias 512 503\n",
      "postnet.convolutions.1.convolution1d.weight 1310720 1287680\n",
      "postnet.convolutions.1.batch_normalization.weight 512 512\n",
      "postnet.convolutions.1.batch_normalization.bias 512 512\n",
      "postnet.convolutions.2.convolution1d.bias 512 503\n",
      "postnet.convolutions.2.convolution1d.weight 1310720 1287680\n",
      "postnet.convolutions.2.batch_normalization.weight 512 512\n",
      "postnet.convolutions.2.batch_normalization.bias 512 512\n",
      "postnet.convolutions.3.convolution1d.bias 512 503\n",
      "postnet.convolutions.3.convolution1d.weight 1310720 1287680\n",
      "postnet.convolutions.3.batch_normalization.weight 512 512\n",
      "postnet.convolutions.3.batch_normalization.bias 512 512\n",
      "postnet.convolutions.4.convolution1d.weight 204800 204800\n",
      "postnet.convolutions.4.convolution1d.bias 80 80\n",
      "postnet.convolutions.4.batch_normalization.weight 80 80\n",
      "postnet.convolutions.4.batch_normalization.bias 80 80\n",
      "coarse_decoder.prenet.linear_layers.0.linear_layer.weight 20480 20400\n",
      "coarse_decoder.prenet.linear_layers.1.linear_layer.weight 65536 65280\n",
      "coarse_decoder.attention_rnn.bias_ih 4096 4056\n",
      "coarse_decoder.attention_rnn.bias_hh 4096 4057\n",
      "coarse_decoder.attention_rnn.weight_ih 3145728 3115008\n",
      "coarse_decoder.attention_rnn.weight_hh 4194304 4154368\n",
      "coarse_decoder.attention.query_layer.bias 128 127\n",
      "coarse_decoder.attention.query_layer.weight 131072 130048\n",
      "coarse_decoder.attention.key_layer.weight 21504 21376\n",
      "coarse_decoder.attention.static_filter_conv.weight 168 168\n",
      "coarse_decoder.attention.static_filter_layer.weight 1024 1024\n",
      "coarse_decoder.attention.dynamic_filter_layer.weight 1024 1024\n",
      "coarse_decoder.attention.dynamic_filter_layer.bias 128 128\n",
      "coarse_decoder.attention.v.weight 128 128\n",
      "coarse_decoder.decoder_rnn.bias_ih 4096 4062\n",
      "coarse_decoder.decoder_rnn.bias_hh 4096 4070\n",
      "coarse_decoder.decoder_rnn.weight_ih 6291456 6239232\n",
      "coarse_decoder.decoder_rnn.weight_hh 4194304 4167680\n",
      "coarse_decoder.linear_projection.linear_layer.weight 737280 737280\n",
      "coarse_decoder.linear_projection.linear_layer.bias 480 480\n",
      "coarse_decoder.stopnet.1.linear_layer.weight 1504 1504\n",
      "coarse_decoder.stopnet.1.linear_layer.bias 1 1\n"
     ]
    }
   ],
   "source": [
    "for n, p in model.named_parameters():\n",
    "    print(n, p.numel(), p.count_nonzero().item())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "outputs": [],
   "source": [
    "from TTS.utils.generic_utils import count_parameters"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'total_removed' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_2038492/3922608831.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mtotal_removed\u001B[0m \u001B[0;34m/\u001B[0m \u001B[0mcount_parameters\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m: name 'total_removed' is not defined"
     ]
    }
   ],
   "source": [
    "total_removed / count_parameters(model)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "outputs": [],
   "source": [
    "from TTS.utils.io import save_model\n",
    "output_path = os.path.join(BASE_DIR, 'trim', 'sparsity_5', 'checkpoint_100000.pth.tar')\n",
    "save_model(config, model,\n",
    "                   optimizer=None,\n",
    "                   scheduler=None,\n",
    "                   scaler=None,\n",
    "                   current_step=100000,\n",
    "                   epoch=None,\n",
    "                   output_path=output_path)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "822ce188d9bce5372c4adbb11364eeb49293228c2224eb55307f4664778e7f56"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}