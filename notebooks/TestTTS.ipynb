{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a notebook to generate mel-spectrograms from a TTS model to be used in a Vocoder training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import importlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm as tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from TTS.tts.datasets.dataset import TTSDataset\n",
    "from TTS.tts.layers.losses import L1LossMasked\n",
    "from TTS.utils.audio import AudioProcessor\n",
    "from TTS.config import load_config\n",
    "from TTS.tts.utils.visual import plot_spectrogram\n",
    "from TTS.tts.utils.helpers import sequence_mask\n",
    "from TTS.tts.models import setup_model\n",
    "from TTS.tts.utils.text.symbols import make_symbols, symbols, phonemes\n",
    "from TTS.tts.utils.synthesis import inv_spectrogram\n",
    "from TTS.utils.generic_utils import get_npy_path\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0'\n",
    "from matplotlib import pyplot as plt\n",
    "plt.style.use('dark_background')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = '/home/perry/PycharmProjects/TTS/recipes/ljspeech/prune/'\n",
    "DATA_DIR = \"/home/perry/PycharmProjects/TTS/recipes/ljspeech/LJSpeech-1.1/\"\n",
    "TEST_CSV = DATA_DIR + \"splits/test.csv\"\n",
    "MODEL_DIR = BASE_DIR + \"tacotron2_nomask/baseline/\"\n",
    "CONFIG_PATH = MODEL_DIR + \"config.json\"\n",
    "STEPS = 100000\n",
    "MODEL_FILE = MODEL_DIR + f\"checkpoint_{STEPS}.pth.tar\"\n",
    "BATCH_SIZE = 32\n",
    "MEL_OUT_DIR = MODEL_DIR + f\"mel_{STEPS}/\"\n",
    "FULL_MEL_OUT_DIR = MODEL_DIR + f\"full_mel_{STEPS}/\"\n",
    "WAV_OUT_DIR = MODEL_DIR + f\"wav_gl_{STEPS}/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "DRY_RUN = False   # if False, does not generate output files, only computes loss and visuals.\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print(\" > CUDA enabled: \", use_cuda)\n",
    "\n",
    "C = load_config(CONFIG_PATH)\n",
    "C.audio['do_trim_silence'] = False  # IMPORTANT!!!!!!!!!!!!!!! disable to align mel specs with the wav files\n",
    "ap = AudioProcessor(**C.audio)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "C.max_decoder_steps = 1000"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      " > Using model: tacotron2\n",
      " > Model's reduction rate `r` is set to: 1\n"
     ]
    }
   ],
   "source": [
    "print(C['r'])\n",
    "# if the vocabulary was passed, replace the default\n",
    "if 'characters' in C and C['characters']:\n",
    "    print('Using custom chars')\n",
    "    symbols, phonemes = make_symbols(**C.characters)\n",
    "MODEL_FILE = '/home/perry/PycharmProjects/TTS/recipes/ljspeech/prune/tacotron2_nomask/snip/sparsity_20/checkpoint_100000.pth.tar'\n",
    "# load the model\n",
    "num_chars = len(phonemes) if C.use_phonemes else len(symbols)\n",
    "# TODO: multiple speaker\n",
    "model = setup_model(C)\n",
    "model.load_checkpoint(C, MODEL_FILE, eval=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "model = model.cuda()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding.weight 102400 22047\n",
      "encoder.convolutions.0.convolution1d.weight 1310720 1253892\n",
      "encoder.convolutions.0.convolution1d.bias 512 0\n",
      "encoder.convolutions.0.batch_normalization.weight 512 512\n",
      "encoder.convolutions.0.batch_normalization.bias 512 512\n",
      "encoder.convolutions.1.convolution1d.weight 1310720 1261157\n",
      "encoder.convolutions.1.convolution1d.bias 512 0\n",
      "encoder.convolutions.1.batch_normalization.weight 512 512\n",
      "encoder.convolutions.1.batch_normalization.bias 512 512\n",
      "encoder.convolutions.2.convolution1d.weight 1310720 1268294\n",
      "encoder.convolutions.2.convolution1d.bias 512 0\n",
      "encoder.convolutions.2.batch_normalization.weight 512 512\n",
      "encoder.convolutions.2.batch_normalization.bias 512 512\n",
      "encoder.lstm.weight_ih_l0 524288 520618\n",
      "encoder.lstm.weight_hh_l0 262144 250377\n",
      "encoder.lstm.bias_ih_l0 1024 1021\n",
      "encoder.lstm.bias_hh_l0 1024 1018\n",
      "encoder.lstm.weight_ih_l0_reverse 524288 518723\n",
      "encoder.lstm.weight_hh_l0_reverse 262144 243394\n",
      "encoder.lstm.bias_ih_l0_reverse 1024 1016\n",
      "encoder.lstm.bias_hh_l0_reverse 1024 1020\n",
      "decoder.prenet.linear_layers.0.linear_layer.weight 20480 17439\n",
      "decoder.prenet.linear_layers.1.linear_layer.weight 65536 51921\n",
      "decoder.attention_rnn.weight_ih 3145728 2687977\n",
      "decoder.attention_rnn.weight_hh 4194304 3603798\n",
      "decoder.attention_rnn.bias_ih 4096 4024\n",
      "decoder.attention_rnn.bias_hh 4096 4018\n",
      "decoder.attention.query_layer.weight 131072 95551\n",
      "decoder.attention.query_layer.bias 128 125\n",
      "decoder.attention.key_layer.weight 21504 18088\n",
      "decoder.attention.static_filter_conv.weight 168 165\n",
      "decoder.attention.static_filter_layer.weight 1024 1018\n",
      "decoder.attention.dynamic_filter_layer.weight 1024 843\n",
      "decoder.attention.dynamic_filter_layer.bias 128 127\n",
      "decoder.attention.v.weight 128 127\n",
      "decoder.decoder_rnn.weight_ih 6291456 5591223\n",
      "decoder.decoder_rnn.weight_hh 4194304 3357061\n",
      "decoder.decoder_rnn.bias_ih 4096 4059\n",
      "decoder.decoder_rnn.bias_hh 4096 4054\n",
      "decoder.linear_projection.linear_layer.weight 122880 122805\n",
      "decoder.linear_projection.linear_layer.bias 80 80\n",
      "decoder.stopnet.1.linear_layer.weight 1104 1104\n",
      "decoder.stopnet.1.linear_layer.bias 1 1\n",
      "postnet.convolutions.0.convolution1d.weight 204800 203329\n",
      "postnet.convolutions.0.convolution1d.bias 512 0\n",
      "postnet.convolutions.0.batch_normalization.weight 512 512\n",
      "postnet.convolutions.0.batch_normalization.bias 512 512\n",
      "postnet.convolutions.1.convolution1d.weight 1310720 1300090\n",
      "postnet.convolutions.1.convolution1d.bias 512 0\n",
      "postnet.convolutions.1.batch_normalization.weight 512 512\n",
      "postnet.convolutions.1.batch_normalization.bias 512 512\n",
      "postnet.convolutions.2.convolution1d.weight 1310720 1297299\n",
      "postnet.convolutions.2.convolution1d.bias 512 0\n",
      "postnet.convolutions.2.batch_normalization.weight 512 512\n",
      "postnet.convolutions.2.batch_normalization.bias 512 512\n",
      "postnet.convolutions.3.convolution1d.weight 1310720 1296728\n",
      "postnet.convolutions.3.convolution1d.bias 512 0\n",
      "postnet.convolutions.3.batch_normalization.weight 512 512\n",
      "postnet.convolutions.3.batch_normalization.bias 512 512\n",
      "postnet.convolutions.4.convolution1d.weight 204800 204206\n",
      "postnet.convolutions.4.convolution1d.bias 80 0\n",
      "postnet.convolutions.4.batch_normalization.weight 80 80\n",
      "postnet.convolutions.4.batch_normalization.bias 80 80\n",
      "coarse_decoder.prenet.linear_layers.0.linear_layer.weight 20480 15059\n",
      "coarse_decoder.prenet.linear_layers.1.linear_layer.weight 65536 39739\n",
      "coarse_decoder.attention_rnn.weight_ih 3145728 2097049\n",
      "coarse_decoder.attention_rnn.weight_hh 4194304 2641120\n",
      "coarse_decoder.attention_rnn.bias_ih 4096 3780\n",
      "coarse_decoder.attention_rnn.bias_hh 4096 3783\n",
      "coarse_decoder.attention.query_layer.weight 131072 30482\n",
      "coarse_decoder.attention.query_layer.bias 128 100\n",
      "coarse_decoder.attention.key_layer.weight 21504 8975\n",
      "coarse_decoder.attention.static_filter_conv.weight 168 166\n",
      "coarse_decoder.attention.static_filter_layer.weight 1024 881\n",
      "coarse_decoder.attention.dynamic_filter_layer.weight 1024 860\n",
      "coarse_decoder.attention.dynamic_filter_layer.bias 128 106\n",
      "coarse_decoder.attention.v.weight 128 128\n",
      "coarse_decoder.decoder_rnn.weight_ih 6291456 4453069\n",
      "coarse_decoder.decoder_rnn.weight_hh 4194304 2342794\n",
      "coarse_decoder.decoder_rnn.bias_ih 4096 3861\n",
      "coarse_decoder.decoder_rnn.bias_hh 4096 3871\n",
      "coarse_decoder.linear_projection.linear_layer.weight 737280 732485\n",
      "coarse_decoder.linear_projection.linear_layer.bias 480 480\n",
      "coarse_decoder.stopnet.1.linear_layer.weight 1504 1504\n",
      "coarse_decoder.stopnet.1.linear_layer.bias 1 1\n"
     ]
    }
   ],
   "source": [
    "for p, tensor in model.named_parameters():\n",
    "    print(p, tensor.numel(), int(tensor.count_nonzero()))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " > DataLoader initialization\n",
      " | > Inputs used: phonemes, mel\n",
      " | > Number of instances : 1310\n",
      " | > Max length sequence: 166\n",
      " | > Min length sequence: 12\n",
      " | > Avg length sequence: 93.27099236641222\n",
      " | > Num. instances discarded by max-min (max=2147483647, min=0) seq limits: 0\n",
      " | > Batch group size: 0.\n"
     ]
    }
   ],
   "source": [
    "test_items = pd.read_csv(TEST_CSV).values.tolist()\n",
    "dataset = TTSDataset(\n",
    "    C.dataset_configs[0],\n",
    "    ap,\n",
    "    test_items,\n",
    "    use_phonemes=C.use_phonemes,\n",
    "    use_mel=C.use_mel,\n",
    "    enable_eos_bos=C.enable_eos_bos,\n",
    ")\n",
    "dataset.sort_and_filter_items()\n",
    "loader = DataLoader(\n",
    "    dataset, batch_size=BATCH_SIZE, num_workers=4, collate_fn=dataset.collate_fn, shuffle=False, drop_last=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "with torch.no_grad():\n",
    "    os.makedirs(FULL_MEL_OUT_DIR, exist_ok=True)\n",
    "    os.makedirs(MEL_OUT_DIR, exist_ok=True)\n",
    "    os.makedirs(WAV_OUT_DIR, exist_ok=True)\n",
    "    wav_filenames = []\n",
    "    output_lengths_all = []\n",
    "    mel_gt_lengths_all = []\n",
    "    for batch in loader:\n",
    "        char_ids = batch[\"char_ids\"]\n",
    "        wav_path = batch[\"wav_path\"]\n",
    "        wav_filenames.extend([Path(wav_file).stem for wav_file in wav_path])\n",
    "        mel_gts = batch[\"mel\"]\n",
    "        mel_gt_lengths = batch[\"mel_lengths\"]\n",
    "        mel_gt_lengths_all.extend(mel_gt_lengths.tolist())\n",
    "\n",
    "        # dispatch data to GPU\n",
    "        if use_cuda:\n",
    "            char_ids = char_ids.cuda()\n",
    "\n",
    "        results = model.inference(char_ids)\n",
    "\n",
    "        model_outputs = results['model_outputs']\n",
    "        decoder_outputs = results['decoder_outputs']\n",
    "        alignments = results['alignments']\n",
    "        stop_tokens = results['stop_tokens']\n",
    "\n",
    "        output_lengths = torch.sum(stop_tokens < 0.5, dim=1).squeeze()\n",
    "        output_lengths_all.extend(output_lengths.tolist())\n",
    "\n",
    "        for model_output, output_length, mel_gt_length, wav_file in zip(model_outputs, output_lengths, mel_gt_lengths, wav_path):\n",
    "            # plot posnet output\n",
    "            model_output = model_output[:output_length, :]\n",
    "            spectrogram_full = model_output.cpu().numpy().squeeze().T\n",
    "            np.save(get_npy_path(FULL_MEL_OUT_DIR, wav_file), spectrogram_full)\n",
    "            # model_out = model_output[:mel_gt_length, :]\n",
    "            # spectrogram = model_out.cpu().numpy().squeeze().T\n",
    "            # np.save(get_npy_path(MEL_OUT_DIR, wav_file), spectrogram)\n",
    "\n",
    "            # wav = ap.inv_melspectrogram(spectrogram)\n",
    "            # ap.save_wav(wav, os.path.join(WAV_OUT_DIR, os.path.basename(wav_file)))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# plt.imshow(spectrogram, aspect=\"auto\", origin=\"lower\", cmap='viridis')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "length_data = pd.DataFrame(data=list(zip(mel_gt_lengths, output_lengths)), columns=['mel_gt_length', 'output_length'], index=wav_filenames)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "length_data.to_csv(os.path.join(MEL_OUT_DIR, 'length_data.csv'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "stop_targets = batch['stop_targets']\n",
    "stop_target_lengths = batch['mel_lengths']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "stop_targets.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "id_lengths = batch[\"id_lengths\"].cuda()\n",
    "mel_gts = mel_gts.cuda()\n",
    "mel_lengths = mel_lengths.cuda()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "outputs = model.forward(char_ids, id_lengths, mel_specs=mel_gts, mel_lengths=mel_lengths)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(torch.sum(1-stop_targets, dim=1))\n",
    "print(mel_lengths)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Generate model outputs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "file_idxs = []\n",
    "metadata = []\n",
    "losses = []\n",
    "postnet_losses = []\n",
    "criterion = L1LossMasked(seq_len_norm=C.seq_len_norm)\n",
    "with torch.no_grad():\n",
    "    for data in tqdm(loader):\n",
    "        # setup input data\n",
    "        char_ids = data[\"char_ids\"]\n",
    "        id_lengths = data[\"id_lengths\"]\n",
    "        linear_input = data[\"linear\"]\n",
    "        stop_targets = data[\"stop_targets\"]\n",
    "        wav_path = data[\"wav_path\"]\n",
    "\n",
    "        # dispatch data to GPU\n",
    "        if use_cuda:\n",
    "            char_ids = char_ids.cuda()\n",
    "            id_lengths = id_lengths.cuda()\n",
    "            mel_input = mel_input.cuda()\n",
    "            mel_lengths = mel_lengths.cuda()\n",
    "\n",
    "        mask = sequence_mask(id_lengths)\n",
    "        mel_outputs, postnet_outputs, alignments, stop_tokens = model.forward(char_ids, id_lengths)\n",
    "        \n",
    "        # compute loss\n",
    "        loss = criterion(mel_outputs, mel_input, mel_lengths)\n",
    "        loss_postnet = criterion(postnet_outputs, mel_input, mel_lengths)\n",
    "        losses.append(loss.item())\n",
    "        postnet_losses.append(loss_postnet.item())\n",
    "\n",
    "        mels = postnet_outputs.detach().cpu().numpy()\n",
    "        alignments = alignments.detach().cpu().numpy()\n",
    "\n",
    "        if not DRY_RUN:\n",
    "            os.makedirs(MEL_OUT_DIR, exist_ok=True)\n",
    "            os.makedirs(WAV_OUT_DIR, exist_ok=True)\n",
    "            wavs_gl = apply_griffin_lim(mels, mel_lengths)\n",
    "            for idx in range(char_ids.shape[0]):\n",
    "                wav_file = wav_path[idx]\n",
    "\n",
    "                # save TTS mel\n",
    "                mel = mels[idx]\n",
    "                mel_length = mel_lengths[idx]\n",
    "                mel = mel[:mel_length, :].T\n",
    "                np.save(get_npy_path(MEL_OUT_DIR, wav_file), mel)\n",
    "\n",
    "\n",
    "\n",
    "    print(np.mean(losses))\n",
    "    print(np.mean(postnet_losses))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Sanity Check"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "spec = np.load('/home/perry/PycharmProjects/TTS/recipes/ljspeech/prune/coqui_tts-20220204_1907-aa986857/sparsity_90/full_mel_100000/LJ025-0110.npy')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_spectrogram(spec.T, ap)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# plot decoder output\n",
    "print(postnet_outputs.shape)\n",
    "plot_spectrogram(postnet_outputs, ap)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# plot GT specgrogram\n",
    "print(mel_gts[idx].shape)\n",
    "plot_spectrogram(mel_gts[idx], ap)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# postnet, decoder diff\n",
    "from matplotlib import pylab as plt\n",
    "mel_diff = mel_decoder - mel_postnet\n",
    "plt.figure(figsize=(16, 10))\n",
    "plt.imshow(abs(mel_diff[:mel_lengths[idx],:]).T,aspect=\"auto\", origin=\"lower\");\n",
    "plt.colorbar()\n",
    "plt.tight_layout()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# PLOT GT SPECTROGRAM diff\n",
    "from matplotlib import pylab as plt\n",
    "mel_diff2 = mel_truth.T - mel_decoder\n",
    "plt.figure(figsize=(16, 10))\n",
    "plt.imshow(abs(mel_diff2).T,aspect=\"auto\", origin=\"lower\");\n",
    "plt.colorbar()\n",
    "plt.tight_layout()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# PLOT GT SPECTROGRAM diff\n",
    "from matplotlib import pylab as plt\n",
    "mel = postnet_outputs[idx]\n",
    "mel_diff2 = mel_truth.T - mel[:mel_truth.shape[1]]\n",
    "plt.figure(figsize=(16, 10))\n",
    "plt.imshow(abs(mel_diff2).T,aspect=\"auto\", origin=\"lower\");\n",
    "plt.colorbar()\n",
    "plt.tight_layout()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "822ce188d9bce5372c4adbb11364eeb49293228c2224eb55307f4664778e7f56"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}